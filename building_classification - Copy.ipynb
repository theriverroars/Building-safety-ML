{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# import required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure OpenCV is installed\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure matplotlib is installed\n",
        "#!pip install matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define directories for training and test data\n",
        "train_dir = r\"C:\\Users\\ganga\\Documents\\IISc Coursework\\ML4CPS\\Project1\\Project 1 Data\\Project 1 Data\\Train_Data\"\n",
        "test_dir = r\"C:\\Users\\ganga\\Documents\\IISc Coursework\\ML4CPS\\Project1\\Project 1 Data\\Project 1 Data\\Test_Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Load and Preprocess Training Data\n",
        "IMG_SIZ = (400, 300)\n",
        "#IMG_SIZE = (400, 300)\n",
        "ALLOWED_EXTENSIONS = {'.jpg', '.jpeg', '.png'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load training data\n",
        "def load_data(train_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'S': 5}\n",
        "\n",
        "    for folder in os.listdir(train_dir):\n",
        "        folder_path = os.path.join(train_dir, folder)\n",
        "        if os.path.isdir(folder_path) and folder in label_map:\n",
        "            for img_name in os.listdir(folder_path):\n",
        "                if not any(img_name.lower().endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
        "                    continue\n",
        "                img_path = os.path.join(folder_path, img_name)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, IMG_SIZ)\n",
        "                    images.append(img)\n",
        "                    labels.append(label_map[folder])\n",
        "    return np.array(images), np.array(labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2516 images.\n",
            "Shape of images array: (2516, 300, 400, 3)\n",
            "Shape of labels array: (2516,)\n"
          ]
        }
      ],
      "source": [
        "# Load training data\n",
        "if os.path.exists(train_dir):\n",
        "    images, labels = load_data(train_dir)\n",
        "    print(f\"Loaded {len(images)} images.\")\n",
        "    print(f\"Shape of images array: {images.shape}\")\n",
        "    print(f\"Shape of labels array: {labels.shape}\")\n",
        "\n",
        "# Step 3: Prepare Data for Training\n",
        "labels = labels - 1  # Adjust labels to be 0-based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,  # Reduce rotation range\n",
        "    width_shift_range=0.2,  # Reduce shift range\n",
        "    height_shift_range=0.2,  # Reduce shift range\n",
        "    shear_range=0.2,  # Reduce shear range\n",
        "    zoom_range=0.2,  # Reduce zoom\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    rescale=1./255\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 43.9 MiB for an array with shape (32, 300, 400, 3) and data type float32",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m augmented_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m augmented_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ganga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:112\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m     index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ganga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:648\u001b[0m, in \u001b[0;36mNumpyArrayIterator._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_batches_of_transformed_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m, index_array):\n\u001b[1;32m--> 648\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[0;32m    652\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[j]\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 43.9 MiB for an array with shape (32, 300, 400, 3) and data type float32"
          ]
        }
      ],
      "source": [
        "#Add augmented data to training set\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "\n",
        "\n",
        "for X_batch, y_batch in datagen.flow(images, labels, batch_size=32):\n",
        "    augmented_images.append(X_batch)\n",
        "    augmented_labels.append(y_batch)\n",
        "    if len(augmented_images) * len(X_batch) >= 5000:\n",
        "        break\n",
        "augmented_images = np.concatenate(augmented_images)\n",
        "augmented_labels = np.concatenate(augmented_labels)\n",
        "X_train = np.concatenate([X_train, augmented_images])\n",
        "y_train = np.concatenate([y_train, augmented_labels])\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model architecture\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),  # Reduce neurons in dense layer\n",
        "    Dropout(0.3),  # Reduce dropout rate to retain more information\n",
        "    Dense(256, activation='relu'),  # Additional dense layer\n",
        "    Dropout(0.3),\n",
        "    Dense(5, activation='softmax')  # Assuming 5 classes\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define model architecture\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    \n",
        "    pout(0.3),\n",
        "    Dense(5, activation='softmax')  # Assuming 5 classes\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Compile the model with AdamW optimizer\n",
        "model.compile(optimizer=AdamW(learning_rate=0.00005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Training the Model\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00005)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute class weights (optional: you can experiment with and without this)\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "batch_size = 10  # Use a smaller batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ganga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 753ms/step - accuracy: 0.2456 - loss: 1.9624 - val_accuracy: 0.0734 - val_loss: 1.7107 - learning_rate: 5.0000e-05\n",
            "Epoch 2/40\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 803ms/step - accuracy: 0.2918 - loss: 1.6920 - val_accuracy: 0.3710 - val_loss: 1.5876 - learning_rate: 5.0000e-05\n",
            "Epoch 3/40\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 764ms/step - accuracy: 0.3083 - loss: 1.6187 - val_accuracy: 0.1528 - val_loss: 1.6177 - learning_rate: 5.0000e-05\n",
            "Epoch 4/40\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 765ms/step - accuracy: 0.3551 - loss: 1.5662 - val_accuracy: 0.2183 - val_loss: 1.6016 - learning_rate: 5.0000e-05\n",
            "Epoch 5/40\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 800ms/step - accuracy: 0.3870 - loss: 1.4566 - val_accuracy: 0.1290 - val_loss: 1.6935 - learning_rate: 5.0000e-05\n",
            "Epoch 6/40\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 813ms/step - accuracy: 0.3852 - loss: 1.5043 - val_accuracy: 0.1528 - val_loss: 1.7013 - learning_rate: 5.0000e-05\n",
            "Epoch 7/40\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 768ms/step - accuracy: 0.3934 - loss: 1.4857 - val_accuracy: 0.1885 - val_loss: 1.7256 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x2007f847200>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model with data augmentation, class weights, and callbacks\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "          validation_data=val_datagen.flow(X_val, y_val),\n",
        "          epochs=50,  # Increase epochs for better convergence\n",
        "          class_weight=class_weights,  # Experiment with and without this\n",
        "          callbacks=[reduce_lr, early_stopping])  # Add early stopping to prevent overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 175ms/step - accuracy: 0.2949 - loss: 1.6977\n",
            "Validation accuracy: 0.28\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the validation set and find accuracy\n",
        "loss, accuracy = model.evaluate(val_datagen(X_val, y_val))\n",
        "print(f\"Validation accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ_OKloLWrum",
        "outputId": "e60faf37-0336-492d-bd73-c356430f3f1e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 6: Load and Preprocess Test Data with Missing Image Handling\n",
        "def load_test_data(test_dir):\n",
        "    test_images = []\n",
        "    test_ids = []\n",
        "    missing_images = []\n",
        "    for img_name in os.listdir(test_dir):\n",
        "        img_path = os.path.join(test_dir, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (128, 128))\n",
        "            test_images.append(img)\n",
        "            test_ids.append(img_name.split('.')[0])\n",
        "        else:\n",
        "            print(f\"Warning: Could not load image {img_name}\")\n",
        "            missing_images.append(img_name)\n",
        "\n",
        "    print(f\"Loaded {len(test_images)} images.\")\n",
        "    print(f\"Missing images: {missing_images}\")\n",
        "\n",
        "    return np.array(test_images), test_ids, missing_images\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Could not load image .DS_Store\n",
            "Loaded 478 images.\n",
            "Missing images: ['.DS_Store']\n"
          ]
        }
      ],
      "source": [
        "# Load test data\n",
        "test_images, test_ids, missing_images = load_test_data(test_dir)\n",
        "\n",
        "# Normalize test data\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 231ms/step\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Make Predictions\n",
        "predictions = model.predict(test_images)\n",
        "predicted_classes = np.argmax(predictions, axis=1) + 1  # Adjust to match original labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected 479 images, but loaded 478. Adding placeholder rows for missing images.\n"
          ]
        }
      ],
      "source": [
        "# Ensure predicted_classes has 479 entries\n",
        "if len(predicted_classes) < 479:\n",
        "    print(f\"Expected 479 images, but loaded {len(test_ids)}. Adding placeholder rows for missing images.\")\n",
        "\n",
        "    # Initialize predicted_classes if not already done\n",
        "    predicted_classes = np.array(predicted_classes)\n",
        "\n",
        "    # Add placeholder predictions (e.g., class 1) to match the expected number of rows\n",
        "    for missing_img in missing_images:\n",
        "        test_ids.append(missing_img.split('.')[0])\n",
        "        predicted_classes = np.append(predicted_classes, [1])  # Default prediction as class 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Create Submission File\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_ids,\n",
        "    'Predictions': predicted_classes\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission file created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Save submission variable as a .csv file in the current working directory\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file created successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
