{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implementating EfficeintNETBO for feature extraction ,for visulaizinthose feature we are using PCA ,then on extracted features we have implemented random forest"
      ],
      "metadata": {
        "id": "fT8--klFeWP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC8Gml8XeRaO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Step 1: Mount Google Drive to access data\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define directories for training and test data\n",
        "train_dir = '/content/drive/My Drive/Project 1 Data (2)/Project 1 Data/Train_Data'\n",
        "test_dir = '/content/drive/My Drive/Project 1 Data (2)/Project 1 Data/Test_Data'\n",
        "\n",
        "# Step 2: Load and Preprocess Training Data\n",
        "IMG_SIZE = (224, 224)\n",
        "ALLOWED_EXTENSIONS = {'.jpg', '.jpeg', '.png'}\n",
        "\n",
        "# Function to load training data\n",
        "def load_data(train_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'S': 5}\n",
        "\n",
        "    for folder in os.listdir(train_dir):\n",
        "        folder_path = os.path.join(train_dir, folder)\n",
        "        if os.path.isdir(folder_path) and folder in label_map:\n",
        "            for img_name in os.listdir(folder_path):\n",
        "                if not any(img_name.lower().endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
        "                    continue\n",
        "                img_path = os.path.join(folder_path, img_name)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, IMG_SIZE)\n",
        "                    images.append(img)\n",
        "                    labels.append(label_map[folder])\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load training data\n",
        "if os.path.exists(train_dir):\n",
        "    images, labels = load_data(train_dir)\n",
        "    print(f\"Loaded {len(images)} images.\")\n",
        "    print(f\"Shape of images array: {images.shape}\")\n",
        "    print(f\"Shape of labels array: {labels.shape}\")\n",
        "\n",
        "# Step 3: Prepare Data for Training\n",
        "labels = labels - 1  # Adjust labels to be 0-based\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize image data\n",
        "X_train = X_train / 255.0\n",
        "X_val = X_val / 255.0\n",
        "\n",
        "# Step 4: Apply Data Augmentation to Training Data\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "# Step 5: Define the Feature Extraction Model Using EfficientNetB0\n",
        "base_model = EfficientNetB0(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Unfreeze the last few layers for fine-tuning\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile model for feature extraction\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),  # This layer will give you a feature vector\n",
        "])\n",
        "\n",
        "# Extract features for training and validation data\n",
        "train_features = model.predict(X_train)\n",
        "val_features = model.predict(X_val)\n",
        "\n",
        "# Step 6: Handle Class Imbalance Using Class Weights\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Standardize the extracted features for the classical machine learning models\n",
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "val_features = scaler.transform(val_features)\n",
        "\n",
        "# Step 7: Visualize Extracted Features Using PCA\n",
        "pca = PCA(n_components=2)\n",
        "train_features_pca = pca.fit_transform(train_features)\n",
        "\n",
        "# Plot the PCA-reduced features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(train_features_pca[:, 0], train_features_pca[:, 1], c=y_train, cmap='viridis', s=10)\n",
        "plt.colorbar(label='Class')\n",
        "plt.title('PCA of Extracted Features from EfficientNetB0')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n",
        "\n",
        "# Print information on the extracted features\n",
        "print(\"The extracted features hold information related to high-level representations of the input images, such as edges, textures, and patterns. These features help distinguish between the different image classes. The PCA plot shows how these features cluster based on the class labels, providing insights into their separability.\")\n",
        "\n",
        "# Step 8: Try Random Forest for Classification\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Perform cross-validation and store the accuracy scores\n",
        "rf_cross_val_scores = cross_val_score(rf_classifier, train_features, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Plot Cross-Validation Accuracy\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, 6), rf_cross_val_scores, marker='o', linestyle='-', color='b', label='Random Forest Accuracy')\n",
        "plt.title('Cross-Validation Accuracy of Random Forest')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print Cross-Validation Results\n",
        "print(f\"Random Forest Cross-Validation Accuracy Scores: {rf_cross_val_scores}\")\n",
        "print(f\"Random Forest Mean Validation Accuracy: {np.mean(rf_cross_val_scores)}\")\n",
        "\n",
        "# Step 9: Train the Random Forest Classifier on Full Training Data\n",
        "rf_classifier.fit(train_features, y_train)\n",
        "\n",
        "# Step 10: Predict on Validation Set\n",
        "val_predictions = rf_classifier.predict(val_features)\n",
        "val_accuracy = np.mean(val_predictions == y_val)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Step 11: Visualize Feature Importances (for Random Forest)\n",
        "importances = rf_classifier.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Plot top 10 important features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title('Feature Importance - Random Forest')\n",
        "plt.bar(range(10), importances[indices[:10]], align='center')\n",
        "plt.xticks(range(10), indices[:10])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 12: Load and Preprocess Test Data\n",
        "def load_test_data(test_dir):\n",
        "    test_images = []\n",
        "    test_ids = []\n",
        "    for img_name in os.listdir(test_dir):\n",
        "        img_path = os.path.join(test_dir, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, IMG_SIZE)\n",
        "            test_images.append(img)\n",
        "            test_ids.append(img_name.split('.')[0])\n",
        "\n",
        "    print(f\"Loaded {len(test_images)} test images.\")\n",
        "    return np.array(test_images), test_ids\n",
        "\n",
        "# Load test data\n",
        "test_images, test_ids = load_test_data(test_dir)\n",
        "\n",
        "# Normalize test data\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Extract features from test data using EfficientNetB0\n",
        "test_features = model.predict(test_images)\n",
        "\n",
        "# Standardize the test features\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "# Step 13: Make Predictions on Test Data using Random Forest\n",
        "rf_test_predictions = rf_classifier.predict(test_features)\n",
        "\n",
        "# Adjust predictions to match original label indices\n",
        "rf_predicted_classes = rf_test_predictions + 1\n",
        "\n",
        "# Step 14: Create Submission File\n",
        "rf_submission = pd.DataFrame({\n",
        "    'ID': test_ids,\n",
        "    'Predictions': rf_predicted_classes\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "rf_submission.to_csv('rf_submission.csv', index=False)\n",
        "\n",
        "# Step 15: Download Submission File\n",
        "from google.colab import files\n",
        "files.download('rf_submission.csv')\n"
      ]
    }
  ]
}